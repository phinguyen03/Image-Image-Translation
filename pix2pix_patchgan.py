# -*- coding: utf-8 -*-
"""Pix2Pix_PatchGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17P2XlCsb56Nye5b56PVmpiB52A5HJpTB

## Pix2Pix: Image - Image Translation
Github author: https://github.com/znxlwm/pytorch-pix2pix/tree/3059f2af53324e77089bbcfc31279f01a38c40b8
"""

from google.colab import drive
drive.mount('/content/drive')

import tarfile
import os
import time

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms
from torchvision.utils import save_image
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np

DATA_UNZIP = '/content/dataset/facades.tar.gz'
ROOT_DIR = '/content/dataset1/facades'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def extract_tar_gz(tar_path, extract_to='/content/dataset1/'):
    """
    Giải nén file .tar.gz

    Args:
        tar_path (str): Đường dẫn tới file .tar.gz
        extract_to (str): Thư mục muốn giải nén đến
    """
    if not os.path.exists(tar_path):
        print(f"Không tìm thấy file: {tar_path}")
        return

    with tarfile.open(tar_path, 'r:') as tar:
        tar.extractall(path=extract_to)
        print(f"Đã giải nén thành công tới: {extract_to}")

extract_tar_gz(DATA_UNZIP, '/content/dataset1/')

"""# Dataset"""

open_dir = os.listdir(os.path.join(ROOT_DIR, 'train'))
img_path = os.path.join(os.path.join(ROOT_DIR, 'train'), open_dir[0])

img = Image.open(img_path).convert('RGB')
print(img.size) #w, h

class FacadesDataset(Dataset):
  def __init__(self, root_dir, transform=None, split='train'):
    self.root_dir = os.path.join(root_dir, split)
    self.image_files = sorted(os.listdir(self.root_dir))
    self.transform = transform

  def __len__(self):
    return len(self.image_files)
  def __getitem__(self, index):
    img_path = os.path.join(self.root_dir, self.image_files[index])
    #read image
    img = Image.open(img_path).convert('RGB')
    w, h = img.size
    # Ảnh segmentation mask nằm bên phải → crop từ giữa đến cuối
    input_img = img.crop((w // 2, 0, w, h))       # segmentation mask (input)

    # Ảnh thật nằm bên trái → crop từ đầu đến giữa
    ground_truth = img.crop((0, 0, w // 2, h))    # real image (target)
    if self.transform:
      input_img, ground_truth = self.transform(input_img), self.transform(ground_truth)


    return input_img, ground_truth

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),  # giá trị ảnh [0, 1]
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # chuẩn hóa về [-1, 1]
])

"""# DataLoader"""

train_ds = FacadesDataset(ROOT_DIR, transform, split='train')
test_ds = FacadesDataset(ROOT_DIR, transform, split='test')
#val_ds = FacadesDataset(ROOT_DIR, transform, split='val')

train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)
#val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)
test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)

"""# Model"""

class DoubleConv(nn.Module):
  def __init__(self, in_feature, out_feature, use_dropout=False):
    super(DoubleConv, self).__init__()
    layers = [
          nn.Conv2d(in_feature, out_feature, 3, padding=1),
          nn.InstanceNorm2d(out_feature),
          nn.ReLU(inplace=True),
          nn.Conv2d(out_feature, out_feature, 3, padding=1),
          nn.InstanceNorm2d(out_feature),
          nn.ReLU(inplace=True)
      ]
    if use_dropout:
        layers.append(nn.Dropout(0.5))  # hoặc 0.3 nếu bạn muốn nhẹ hơn
    self.conv = nn.Sequential(*layers)
  def forward(self, x):
    return self.conv(x)

# Generator: UNet
class UNetGenerator(nn.Module):
    def __init__(self, n_channels=3, n_classes=64, base_c=64):
        super(UNetGenerator, self).__init__()
        d = base_c

        # Encoder
        self.down1 = DoubleConv(n_channels, d)            # 3 -> 64
        self.down2 = DoubleConv(d, d * 2)                 # 64 -> 128
        self.down3 = DoubleConv(d * 2, d * 4)             # 128 -> 256
        self.down4 = DoubleConv(d * 4, d * 8)             # 256 -> 512
        self.down5 = DoubleConv(d * 8, d * 8)             # 512 -> 512
        self.down6 = DoubleConv(d * 8, d * 8)             # 512 -> 512
        self.down7 = DoubleConv(d * 8, d * 8, use_dropout=True)  # 512 -> 512
        self.down8 = DoubleConv(d * 8, d * 8)             #512 -> 512
        self.pool = nn.MaxPool2d(2)

        # Decoder
        self.up1 = nn.ConvTranspose2d(d * 8, d * 8, 2, 2)
        self.conv1 = DoubleConv(d * 8 + d * 8, d * 8, use_dropout=True)

        self.up2 = nn.ConvTranspose2d(d * 8, d * 8, 2, 2)
        self.conv2 = DoubleConv(d * 8 + d * 8, d * 8, use_dropout=True)

        self.up3 = nn.ConvTranspose2d(d * 8, d * 8, 2, 2)
        self.conv3 = DoubleConv(d * 8 + d * 8, d * 8, use_dropout=True)

        self.up4 = nn.ConvTranspose2d(d * 8, d * 4, 2, 2)
        self.conv4 = DoubleConv(d * 4 + d * 8, d * 4)

        self.up5 = nn.ConvTranspose2d(d * 4, d * 2, 2, 2)
        self.conv5 = DoubleConv(d * 2 + d * 4, d * 2)

        self.up6 = nn.ConvTranspose2d(d * 2, d, 2, 2)
        self.conv6 = DoubleConv(d + d * 2, d)

        self.up7 = nn.ConvTranspose2d(d, d, 2, 2)
        self.conv7 = DoubleConv(d + d, d)

        self.out = nn.Conv2d(d, n_channels, kernel_size=1)

    def forward(self, x):
        x1 = self.down1(x)          # 64
        x2 = self.down2(self.pool(x1))  # 128
        x3 = self.down3(self.pool(x2))  # 256
        x4 = self.down4(self.pool(x3))  # 512
        x5 = self.down5(self.pool(x4))  # 512
        x6 = self.down6(self.pool(x5))  # 512
        x7 = self.down7(self.pool(x6))  # 512
        x8 = self.down8(self.pool(x7))

        x = self.up1(x8)
        x = self.conv1(torch.cat([x, x7], dim=1))

        x = self.up2(x)
        x = self.conv2(torch.cat([x, x6], dim=1))

        x = self.up3(x)
        x = self.conv3(torch.cat([x, x5], dim=1))

        x = self.up4(x)
        x = self.conv4(torch.cat([x, x4], dim=1))

        x = self.up5(x)
        x = self.conv5(torch.cat([x, x3], dim=1))

        x = self.up6(x)
        x = self.conv6(torch.cat([x, x2], dim=1))

        x = self.up7(x)
        x = self.conv7(torch.cat([x, x1], dim=1))

        return torch.tanh(self.out(x))  # Chuẩn Pix2Pix normalize [-1,1]

# Discriminator: PatchGAN
class PatchGANDiscriminator(nn.Module):
     # initializers
    def __init__(self, d=64, in_channels=3):
        super(PatchGANDiscriminator, self).__init__()
        self.conv1 = nn.Conv2d(in_channels * 2, d, 4, 2, 1)
        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)
        self.conv2_bn = nn.BatchNorm2d(d * 2)
        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)
        self.conv3_bn = nn.BatchNorm2d(d * 4)
        self.dropout = nn.Dropout(0.3)
        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 1, 1)
        self.conv4_bn = nn.BatchNorm2d(d * 8)
        self.conv5 = nn.Conv2d(d * 8, 1, 4, 1, 1)


    # weight_init
    def weight_init(self, mean, std):
        for m in self._modules:
            normal_init(self._modules[m], mean, std)

    # forward method
    def forward(self, input, label):
        x = torch.cat([input, label], 1)
        x = F.leaky_relu(self.conv1(x), 0.2)
        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)
        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)
        x = self.dropout(x)
        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)
        x = torch.sigmoid(self.conv5(x))
        return x


def normal_init(m, mean, std):
    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):
        m.weight.data.normal_(mean, std)
        m.bias.data.zero_()

import torchvision.models as models


class VGGPerceptualLoss(nn.Module):
    def __init__(self, resize=True):
        super(VGGPerceptualLoss, self).__init__()
        vgg_features = models.vgg19(pretrained=True).features
        self.vgg = nn.Sequential(*list(vgg_features)[:16]).eval()  # lên đến layer relu4_4
        for param in self.vgg.parameters():
            param.requires_grad = False

        self.resize = resize
        self.criterion = nn.L1Loss()

    def forward(self, input, target):
        # Chuẩn hóa như ImageNet (vì VGG được huấn luyện trên đó)
        mean = torch.tensor([0.485, 0.456, 0.406]).to(input.device).view(1, 3, 1, 1)
        std  = torch.tensor([0.229, 0.224, 0.225]).to(input.device).view(1, 3, 1, 1)
        input = (input - mean) / std
        target = (target - mean) / std

        input_features = self.vgg(input)
        target_features = self.vgg(target)
        loss = self.criterion(input_features, target_features)
        return loss

generator = UNetGenerator()
discriminator = PatchGANDiscriminator()

generator.apply(lambda m: normal_init(m, 0.0, 0.02))
discriminator.apply(lambda m: normal_init(m, 0.0, 0.02))

generator = generator.to(device)
discriminator = discriminator.to(device)

adversarial_loss = nn.BCEWithLogitsLoss().cuda()
l1_loss = nn.L1Loss().cuda()
perceptual_loss = VGGPerceptualLoss().cuda()

optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

"""# Training

"""

train_hist = {}
train_hist['D_losses'] = []
train_hist['G_losses'] = []
train_hist['per_epoch_ptimes'] = []

num_epochs = 100
L1_lambda = 100
lambda_percep = 2
#real_label = 0.9
#fake_label = 0.0
print('training start!')
start_time = time.time()
discriminator.train()
generator.train()
for epochs in range(num_epochs):
  D_losses = []
  G_losses = []
  epoch_start_time = time.time()
  num_iter = 0
  torch.cuda.empty_cache()
  for input_img, ground_truth in train_loader:
    # train discriminator D
    discriminator.zero_grad()

    # Thêm noise Gaussian nhỏ vào input
    # noise = torch.randn_like(input_img) * 0.1  # điều chỉnh noise scale nếu cần
    # noisy_input = input_img + noise
    # noisy_input = torch.clamp(noisy_input, 0., 1.)  # giữ trong khoảng hợp lệ

    input_img, ground_truth = input_img.cuda(), ground_truth.cuda()
    #real
    D_result = discriminator(input_img, ground_truth).squeeze()
    #D_real_loss
    D_real_loss = adversarial_loss(D_result, torch.ones(D_result.size()).cuda())
    #fake

    G_result = generator(input_img)
    if G_result.size()[2:] != input_img.size()[2:]:
      G_result = F.interpolate(G_result, size=input_img.shape[2:], mode='bilinear', align_corners=True)

    D_result_fake = discriminator(input_img, G_result.detach()).squeeze()
    D_fake_loss = adversarial_loss(D_result_fake,  torch.zeros(D_result_fake.size()).cuda())

    D_train_loss = (D_real_loss + D_fake_loss) * 0.5
    D_train_loss.backward()
    optimizer_D.step()

    train_hist['D_losses'].append(D_train_loss.item())
    D_losses.append(D_train_loss.item())

    # train generator G
    generator.zero_grad()

    G_result = generator(input_img) #fake image
    D_fake_for_G = discriminator(input_img, G_result).squeeze()
    loss_percep = perceptual_loss(G_result, ground_truth)
    G_train_loss = adversarial_loss(D_fake_for_G, torch.ones(D_fake_for_G.size()).cuda()) + L1_lambda * l1_loss(G_result, ground_truth) + lambda_percep * loss_percep
    G_train_loss.backward()
    optimizer_G.step()

    train_hist['G_losses'].append(G_train_loss.item())

    G_losses.append(G_train_loss.item())

    num_iter += 1
  epoch_end_time = time.time()
  per_epoch_ptime = epoch_end_time - epoch_start_time

  print(f'Epoch [{epochs+1}/{num_epochs}] | D_loss: {sum(D_losses)/len(D_losses):.4f} | G_loss: {sum(G_losses)/len(G_losses):.4f} | Time: {per_epoch_ptime:.2f}s')
  #save state_dict
  if epochs % 2 == 0:
    torch.save({
    'generator': generator.state_dict(),
    'discriminator': discriminator.state_dict(),
    'optimizer_G': optimizer_G.state_dict(),
    'optimizer_D': optimizer_D.state_dict(),
    'epoch': epochs
}, 'checkpoint.pth')

checkpoint = torch.load('/content/checkpoint.pth')
generator.load_state_dict(checkpoint['generator'])
discriminator.load_state_dict(checkpoint['discriminator'])
optimizer_G.load_state_dict(checkpoint['optimizer_G'])
optimizer_D.load_state_dict(checkpoint['optimizer_D'])
start_epoch = checkpoint['epoch'] + 1

generator.to(device)
discriminator.to(device)

train_hist = {}
train_hist['D_losses'] = []
train_hist['G_losses'] = []
train_hist['per_epoch_ptimes'] = []

num_epochs = 150
L1_lambda = 100
lambda_percep = 2
#real_label = 0.9
#fake_label = 0.0
generator.train()
discriminator.train()
start_time = time.time()
for epochs in range(start_epoch, num_epochs):
    D_losses = []
    G_losses = []
    epoch_start_time = time.time()
    num_iter = 0
    torch.cuda.empty_cache()
    for input_img, ground_truth in train_loader:
      # train discriminator D
      discriminator.zero_grad()

      # Thêm noise Gaussian nhỏ vào input
      # noise = torch.randn_like(input_img) * 0.1  # điều chỉnh noise scale nếu cần
      # noisy_input = input_img + noise
      # noisy_input = torch.clamp(noisy_input, 0., 1.)  # giữ trong khoảng hợp lệ

      input_img, ground_truth = input_img.cuda(), ground_truth.cuda()
      #real
      D_result = discriminator(input_img, ground_truth).squeeze()
      #D_real_loss
      D_real_loss = adversarial_loss(D_result, torch.ones(D_result.size()).cuda())
      #fake

      G_result = generator(input_img)
      if G_result.size()[2:] != input_img.size()[2:]:
        G_result = F.interpolate(G_result, size=input_img.shape[2:], mode='bilinear', align_corners=True)

      D_result_fake = discriminator(input_img, G_result.detach()).squeeze()
      D_fake_loss = adversarial_loss(D_result_fake,  torch.zeros(D_result_fake.size()).cuda())

      D_train_loss = (D_real_loss + D_fake_loss) * 0.5
      D_train_loss.backward()
      optimizer_D.step()

      train_hist['D_losses'].append(D_train_loss.item())
      D_losses.append(D_train_loss.item())

      # train generator G
      generator.zero_grad()

      G_result = generator(input_img) #fake image
      D_fake_for_G = discriminator(input_img, G_result).squeeze()
      loss_percep = perceptual_loss(G_result, ground_truth)
      G_train_loss = adversarial_loss(D_fake_for_G, torch.ones(D_fake_for_G.size()).cuda()) + L1_lambda * l1_loss(G_result, ground_truth) + lambda_percep * loss_percep
      G_train_loss.backward()
      optimizer_G.step()

      train_hist['G_losses'].append(G_train_loss.item())

      G_losses.append(G_train_loss.item())

      num_iter += 1
    epoch_end_time = time.time()
    per_epoch_ptime = epoch_end_time - epoch_start_time

    print(f'Epoch [{epochs+1}/{num_epochs}] | D_loss: {sum(D_losses)/len(D_losses):.4f} | G_loss: {sum(G_losses)/len(G_losses):.4f} | Time: {per_epoch_ptime:.2f}s')
    #save state_dict
    if epochs % 2 == 0:
      torch.save({
      'generator': generator.state_dict(),
      'discriminator': discriminator.state_dict(),
      'optimizer_G': optimizer_G.state_dict(),
      'optimizer_D': optimizer_D.state_dict(),
      'epoch': epochs
  }, 'checkpoint1.pth')

end_time = time.time()
total_ptime = end_time - start_time

"""# Testing"""

from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

generator.eval()

save_dir = './generated_test_images'
os.makedirs(save_dir, exist_ok=True)
num_images = 5
count = 0
with torch.no_grad():
    for i, data in enumerate(test_loader):
        inputs, targets = data
        inputs = inputs.to(device)
        targets = targets.to(device)
        targets = (targets + 1) / 2  # chuyển về [0,1]
        outputs = generator(inputs)  # output shape [1, C, H, W], range [-1,1]
        output_img = (outputs + 1) / 2  # chuyển về [0,1]
        real_tensor = targets.squeeze().cpu().detach()  # [C,H,W]
        gen_tensor = output_img.squeeze().cpu().detach()
        # Nếu ảnh có 3 kênh: [C,H,W] → [H,W,C] và chuyển về numpy
        real_np = real_tensor.permute(1, 2, 0).numpy()
        gen_np = gen_tensor.permute(1, 2, 0).numpy()
        # Chuyển về float32 nếu cần, và đảm bảo nằm trong [0,1]
        real_np = real_np.astype('float32')
        gen_np = gen_np.astype('float32')
        # input: ảnh thật và ảnh sinh ra (numpy, dạng [H,W,C] hoặc [H,W])
        psnr_val = psnr(real_np, gen_np, data_range=1.0)    # đo độ nhiễu giữa ảnh dự đoán và ảnh thật (psnr>25db -> tốt)
        ssim_val = ssim(real_np, gen_np, data_range=1.0, channel_axis=-1) # đo độ giống nhau cấu trúc, độ sáng
                                                                              # độ tương phản (ssim > 0.85 -> tốt)
        print(f'Độ nhiễu ảnh: {psnr_val}db')
        print(f'Độ giống nhau: {ssim_val}')
        save_path = os.path.join(save_dir, f"test_{count}.png")
        save_image(output_img[0], save_path)

        count += 1
        if count >= num_images:
            break

print("Done saving test images.")